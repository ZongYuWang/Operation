
### Paramiko模块安装和使用

常见的解决方法都会需要对远程服务器必要的配置，如果远程服务器只有一两台还好说，如果有N台，还需要逐台进行配置，或者需要使用代码进行以上操作时，上面的办法就不太方便了。

使用paramiko可以很好的解决以上问题，比起前面的方法，它仅需要在本地上安装相应的软件（python以及PyCrypto），对远程服务器没有配置要求，对于连接多台服务器，进行复杂的连接操作特别有帮助。


[pip下载地址](https://pypi.python.org/pypi/pip#downloads)  

```ruby
C:\Users\Administrator>d:
D:\>cd python34\pip-9.0.1
D:\python34\pip-9.0.1>python setup.py install
......

Installing pip.exe.manifest script to D:\python34\Scripts

Installed d:\python34\lib\site-packages\pip-9.0.1-py3.4.egg
Processing dependencies for pip==9.0.1
Finished processing dependencies for pip==9.0.1

# 设置环境变量：
;D:\python34\Scripts;
C:\Users\Administrator>pip list
```

```ruby
D:\python34\paramiko-2.4.0>pip install --upgrade setuptools
参考链接：https://wiki.python.org/moin/WindowsCompilers
```

安装paramiko：
```ruby
C:\Users\Administrator>d:
D:\>cd python34\paramiko-2.4.0
D:\python34\paramiko-2.4.0>python setup.py install
running install
......
Installed d:\python34\lib\site-packages\pycparser-2.18-py3.4.egg
Searching for pyasn1==0.4.2
Best match: pyasn1 0.4.2
Processing pyasn1-0.4.2-py3.4.egg
pyasn1 0.4.2 is already the active version in easy-install.pth

Using d:\python34\lib\site-packages\pyasn1-0.4.2-py3.4.egg
Finished processing dependencies for paramiko==2.4.0

```
在pycharm中测试是否安装成功：
```ruby
import paramiko

# 运行
```

#### SSHClient:

- 基于用户名密码连接：
```ruby
import paramiko

# 创建SSH对象
ssh = paramiko.SSHClient()
# 允许连接不在know_hosts文件中的主机
ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
# 连接服务器
ssh.connect(hostname='172.30.105.115', port=22, username='root', password='wangzongyu')

# 执行命令
stdin, stdout, stderr = ssh.exec_command('df')
# 获取命令结果
result = stdout.read()
print(result.decode())

# 关闭连接
ssh.close()
```


- 基于公钥密钥连接：

`SSH秘钥认证：`
```ruby
# 将本机(172.30.105.115)的公钥(私钥不能传给别人)发给需要连接的主机(172.30.105.112)：

[root@localhost ~]# ssh-copy-id "-p2222 baby@172.30.105.112"
The authenticity of host '[172.30.105.112]:2222 ([172.30.105.112]:2222)' can't be established.
RSA key fingerprint is 79:48:d2:8c:ff:19:f9:e5:a2:f5:d6:84:07:17:4c:bd.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '[172.30.105.112]:2222' (RSA) to the list of known hosts.
baby@172.30.105.112's password: 
Now try logging into the machine, with "ssh '-p2222 baby@172.30.105.112'", and check in:

  .ssh/authorized_keys

to make sure we haven't added extra keys that you weren't expecting.

# 使用ssh-copy-id这个命令是为了手动拷贝不能保证一行，这样拷贝能保证公钥在远程主机的这个文件中.ssh/authorized_keys保证是一行

[root@localhost ~]# ssh -p2222 baby@172.30.105.112

```

```ruby
import paramiko

private_key = paramiko.RSAKey.from_private_key_file('id_rsa_105-115')

# 创建SSH对象
ssh = paramiko.SSHClient()
# 允许连接不在know_hosts文件中的主机
ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
# 连接服务器
ssh.connect(hostname='172.30.105.112', port=2222, username='baby', pkey=private_key)

# 执行命令
stdin, stdout, stderr = ssh.exec_command('df')
# 获取命令结果
result = stdout.read()
print(result.decode())

# 关闭连接
ssh.close()
```

#### SFTPClient:

- 基于用户名密码上传下载:

```ruby
import paramiko

transport = paramiko.Transport(('172.30.105.112',2222))
transport.connect(username='baby',password='wangzongyu')

sftp = paramiko.SFTPClient.from_transport(transport)
# 将location.py 上传至服务器 /tmp/test.py
sftp.put('D:\Python27\LICENSE.txt', '/tmp/license_test.py')

#将remove_path 下载到本地 local_path
sftp.get('/tmp/snmpd.conf.bak', 'E:\result')

transport.close()
```


- 基于公钥密钥上传下载:
```ruby
import paramiko
 
private_key = paramiko.RSAKey.from_private_key_file('id_rsa_105-115')
 
transport = paramiko.Transport(('172.30.105.112', 2222))
transport.connect(username='baby', pkey=private_key )
 
sftp = paramiko.SFTPClient.from_transport(transport)
# 将location.py 上传至服务器 /tmp/test.py
sftp.put('/tmp/location.py', '/tmp/test.py')
# 将remove_path 下载到本地 local_path
sftp.get('remove_path', 'local_path')
 
transport.close()
```

### 多线程

计算机中执行任务的最小单元：线程

- 进程：    
优点：同时利用多个CPU，能够同时进行多个操作；     
缺点：耗费资源(重新开辟内存空间，内存是独立的)；      
进程不是越多越好，cpu个数=进程个数；      
计算密集型(用CPU)：使用多进程(必须要先创建一个线程)；      

- 线程：      
优点：共享内存，IO操作时候，创建并发操作；      
缺点：抢占资源；      
线程也不是越多越好，具体案例具体分析，请求上下门切换耗时；      
IO密集型(不用CPU)：使用多线程；    

进程和线程目的：提高执行效率     
- 单进程单线程，主进程、主线程    
- 自定义线程：     
  - 主进程(主线程、子线程)    

同一个进程的线程之间可以直接交流，两个进程想通信，必须通过一个中间代理来实现；      
创建新线程很简单，创建新进程需要对其父进程进行一次克隆；     
一个线程可以控制和操作同一个进程里的其他线程，但是进程只能操作子进程    

#### 多线程实现：
`python多线程不适合CPU密集操作型的任务，适合IO操作密集型的任务`
```ruby

# 方式一(类方式实现)：
import threading
import time

class MyThread(threading.Thread):
    def __init__(self,n):
        super(MyThread,self).__init__() # 因为重构了父类的方法，所以要继承
        self.n = n

    def run(self):  # 这必须是run方法
        print("runnint task",self.n)

t1 = MyThread("t1")
t2 = MyThread("t2")

t1.start()
t2.start()


#方式二：
import threading
import time

def run(n):
    print("task",n)
    time.sleep(2) # 如果不加上sleep，就不能演示出多线程和下面直接写run("t1")的区别

t1 = threading.Thread(target=run,args=('t1',))  #run后面没有括号，元组形式，所以最后也要加一个逗号
t2 = threading.Thread(target=run,args=('t2',))

t1.start()
t2.start()  # t1和t2是同时进行的

#run("t1")  # 通过上面的sleep就能发现这种方式与多线程执行的区别
#run("t2")

```
`主线程不会等待所有的子线程执行完成，各自执行`
```ruby
import threading
import time

class MyThread(threading.Thread):
    def __init__(self,n,sleep_time):
        super(MyThread,self).__init__()
        self.n = n
        self.sleep_time = sleep_time

    def run(self):
        print("runnint task",self.n)
        time.sleep(self.sleep_time)
        print("task done",self.n)

t1 = MyThread("t1",2)
t2 = MyThread("t2",4)

'''
t1.start()
t1.join()  
# 等待t1线程执行完成，在其他语言中是t1.wait()；
# t1.join(2)：表示最多等待2秒
# 加了join的作用是等待第一个线程执行完毕，才会去执行第二个线程，这样多线程之间就变成了串行执行
t2.start()
# 在整个程序的执行最后，其实默认还有一个t2.join，因为只有所有的"并发"线程都执行完成，程序最后才打印"Process finished with exit code 0"
'''

# 为了还是使多线程执行，还要计算执行时间，可以按照下面的方式
t1.start()
t2.start()

t1.join()

print("main thread!",threading.current_thread())  # 本身这个程序就启动了一个线程就是主线程(主线程自己不能看见)，t1和t2分别是两个子线程；可以通过命令查看是不是主线程
# 主线程不会等待所有的子线程执行完成，各自执行
```
`运行结果`
```ruby
runnint task t1
runnint task t2
task done t1
main thread! <_MainThread(MainThread, started 7908)>
task done t2

Process finished with exit code 0
```
`如果想执行50个进程，并且查看总的执行时间呢？`
```ruby
import threading
import time

def run(n):
    print("running task",n)
    time.sleep(2)

start_time = time.time()
t_objs = [] # 存线程实例
for i in range(50):  # python3中这个个数有限制
    t = threading.Thread(target=run,args=('t%s' %i,))
    t.start()
    # t.join() # 不能写在这，这样写就变成串行执行了，需要使用空列表实现
    t_objs.append(t) # 为了不阻塞后面线程的启动，不在这里join，先放一个列表里

for t in t_objs: # 循环线程实例列表，等待所有线程执行完毕
    t.join()

print("-------------- all threads has finished!"，threading.active_count()) # 可以查看活跃的线程个数
print("total run time",time.time() - start_time)

# # t1.start()
# # t2.start()  # t1和t2是同时进行的
#
# run("t1")
# run("t2")

```
`运行结果`
```ruby
running task t0
running task t1
running task t2
running task t3
running task t4
running task t5
......
running task t44
running task t45
running task t46
running task t47
running task t48
running task t49
-------------- all threads has finished!
total run time 2.0111148357391357

```
#### 守护进程：
```ruby
import threading
import time

def run(n):
    print("running task",n)
    time.sleep(2)

start_time = time.time()
t_objs = []
for i in range(50):
    t = threading.Thread(target=run,args=('t%s' %i,))
    t.setDaemon(True) # 把当前线程设置为守护线程，一定要在start之前
    t.start()
    t_objs.append(t)

print("-------------- all threads has finished!",threading.current_thread(),threading.active_count())
print("total run time",time.time() - start_time)
```


#### 线程锁(互斥锁Mutex)：
一个进程下可以启动多个线程，多个线程共享父进程的内存空间，也就意味着每个线程可以访问同一份数据，但是，如果2个线程同时要修改同一份数据会怎样呢？ (如现实中存在的例子：某电商平台中某件物品的库存是1件，如果不做限制，两个人都拿到库存是1这个数据)   

(其实理想情况就是从1加到1000，多个"进程"一起干，最终要快，如果只让一个"线程"自己加，会很慢，但是一起"干"就容易出错)

`需要在Ubuntu系统、Python2环境中运行如下代码，CentOS系统也会正常输出，不要在3.x上运行，3.x上的结果总是正确的，可能是自动加了锁`
```ruby
# -*- coding:utf-8 -*-

import threading
import time

def run(n):
    global  num  # 在每个线程中都获取这个全局变量
    time.sleep(1)
    num +=1 # 对此公共变量进行+1操作

num = 0  # 设定一个共享变量
t_objs = [] #存线程实例
for i in range(1000):
    t = threading.Thread(target=run,args=("t-%s" %i ,))
    t.start()
    t_objs.append(t) 

for t in t_objs: # 等待所有线程执行完毕
    t.join()

print("----------all threads has finished...",threading.current_thread(),threading.active_count())

print("num:",num)

```

`运行结果`
```ruby
root@ubuntu:~# python python_threading.py 
('----------all threads has finished...', <_MainThread(MainThread, started 140420981126912)>, 1)
('num:', 1000)
root@ubuntu:~# python python_threading.py 
('----------all threads has finished...', <_MainThread(MainThread, started 139921205917440)>, 1)
('num:', 999)
root@ubuntu:~# python python_threading.py 
('----------all threads has finished...', <_MainThread(MainThread, started 140428301154048)>, 1)
('num:', 998)

```
正常来讲，这个num结果应该是1000， 但在python 2.7上多运行几次，会发现，最后打印出来的num结果不总是1000，为什么每次运行的结果不一样呢？ 假设两个线程对象t1和t2都要对num=0进行增1运算，t1和t2都各对num修改10次，num的最终的结果应该为20。但是由于是多线程访问，有可能出现下面情况：在num=0时，t1取得num=0。系统此时把t1调度为”sleeping”状态，把t2转换为”running”状态，t2页获得num=0。然后t2对得到的值进行加1并赋给num，使得num=1。然后系统又把t2调度为”sleeping”，把t1转为”running”。线程t1又把它之前得到的0加1后赋值给num。这样，明明t1和t2都完成了1次加1工作，但结果仍然是num=1(多线程执行应该是2)

那解决办法是什么呢？其实每个线程在要修改公共数据时，为了避免自己在还没改完的时候别人也来修改此数据，可以给这个数据加一把锁， 这样其它线程想修改此数据时就必须等待你修改完毕并把锁释放掉后才能再访问此数据。(哪个线程锁定的，哪个线程使用，其他线程只能等待)

- 上述代码加锁：     

`线程锁：只让一个线程运行加锁的那段代码`
```ruby
# -*- coding:utf-8 -*-

import time
import threading

def addNum():
    global num
    print('Thread%s get num:'%i,num )
    time.sleep(1)
    lock.acquire() #修改数据前加锁
    num += 1
    print("Thread%s add_num" %i,num)
    lock.release() #修改后释放

num = 0
thread_list = []
lock = threading.Lock() #生成全局锁
for i in range(5): # 为易懂，测试5个
    t = threading.Thread(target=addNum)
    t.start()
    thread_list.append(t)

for t in thread_list:
    t.join()

print('final num:', num )
```
`运行结果`
```ruby
('Thread0 get num:', 0)
('Thread1 get num:', 0)
('Thread2 get num:', 0)
('Thread3 get num:', 0)
('Thread4 get num:', 0)
('Thread4 add_num', 1)
('Thread4 add_num', 2)
('Thread4 add_num', 3)
('Thread4 add_num', 4)
('Thread4 add_num', 5)
('final num:', 5)
```

#### 全局解释器锁(GIL)：
GIL：存在于Cpython中，称为全局解释器锁，在同一时间只能一个python线程在跑，但这并不是说是串行运行的，他还是"并行"的，CPU在不断的分配cpu时间给每个线程去运行，只是同一时间刻只有一个线程在跑

![](https://github.com/ZongYuWang/image/blob/master/python-thread1.png)

既然用户程序已经自己有锁了，那为什么Cpython还需要GIL呢？加入GIL主要的原因是为了降低程序的开发的复杂度，比如现在的你写python不需要关心内存回收的问题，因为Python解释器帮你自动定期进行内存回收，你可以理解为python解释器里有一个独立的线程，每过一段时间它起wake up做一次全局轮询看看哪些内存数据是可以被清空的，此时你自己的程序 里的线程和 py解释器自己的线程是并发运行的，假设你的线程删除了一个变量，py解释器的垃圾回收线程在清空这个变量的过程中的clearing时刻，可能一个其它线程正好又重新给这个还没来及得清空的内存空间赋值了，结果就有可能新赋值的数据被删除了，为了解决类似的问题，python解释器简单粗暴的加了锁，即当一个线程运行时，其它人都不能动，这样就解决了上述的问题，这可以说是Python早期版本的遗留问题

#### RLock（递归锁）:
`说白了就是在一个大锁中还要再包含子锁`
```ruby
import threading, time

def run1():
    print("grab the first part data")
    lock.acquire()
    global num
    num += 1
    lock.release()
    return num

def run2():
    print("grab the second part data")
    lock.acquire()
    global num2
    num2 += 1
    lock.release()
    return num2

def run3():
    lock.acquire()
    res = run1()
    print('--------between run1 and run2-----')
    res2 = run2()
    lock.release()
    print(res, res2)

num, num2 = 0, 0
lock = threading.RLock()  
#这地如果是threading.Lock()，这个程序就会死锁(实质就是锁太多，用第二把锁去开第一个门)
for i in range(1):
    t = threading.Thread(target=run3)
    t.start()

while threading.active_count() != 1:
    print(threading.active_count())
else:
    print('----all threads done---')
    print(num, num2)
    
# RLock其实就是等于如下形式：
#locks = {
#    door1：key1
#    door2：key2
#}
```

#### 信号量(Semaphore)：
`互斥锁同时只允许一个线程更改数据，而Semaphore是同时允许一定数量的线程更改数据`
```ruby
import threading,time

def run(n):
    semaphore.acquire()
    time.sleep(1)
    print("run the thread: %s\n" %n)
    semaphore.release()

if __name__ == '__main__':

    num= 0
    semaphore  = threading.BoundedSemaphore(5) #最多允许5个线程同时运行
    for i in range(20):
        t = threading.Thread(target=run,args=(i,))
        t.start()

while threading.active_count() != 1:
    pass #print threading.active_count()
else:
    print('----all threads done---')
    print(num)

# 5个5个的执行，5个中每出来一个就再“放”进去一个，完成3个就再放进去3个
```

#### event(事件):
event是线程间通信的机制之一(实现多个进程之间的交互)，一个线程发送一个event信号，其他的线程则等待这个信号，用于主线程控制其他线程的执行，events管理一个flag，这个flag可以使用set()设置成true或者使用clear()重置为false，wait()则用于阻塞，在flag为true之前，flag默认为false
- event.wait([timeout])：阻塞线程，直到event对象内部标识位被设为true或超时；（红灯）
- event.set()：将标识位设为true；（让灯变成绿灯）
- event.clear()：将标识位设置为false；（让灯变成红灯）
- event.is_set()：判断标识位是否为true；

实例一：
```ruby
# -*- coding:utf-8 -*-

import threading

def do(event):
    print('start')
    event.wait()
    print('execute')


event_obj = threading.Event()
for i in range(10):
    t = threading.Thread(target=do, args=(event_obj,))
    t.start()

event_obj.clear()
inp = input('input:')
if inp == 'true':
    event_obj.set()
```
`运行结果：`
```ruby
start
start
start
start
start
start
start
start
start
start
input:true
execute
execute
execute
execute
execute
execute
execute
execute
execute
execute
```
实例二：
```ruby
import time
import threading


event = threading.Event()

def lighter():
    count = 0
    event.set() #先设置绿灯
    while True:
        if count >5 and count < 10: #改成红灯
            event.clear() #把标志位清了
            print("\033[41;1mred light is on....\033[0m")
        elif count >10:
            event.set() #变绿灯
            count = 0
        else:
            print("\033[42;1mgreen light is on....\033[0m")
        time.sleep(1)
        count +=1

def car(name):
    while True:
        if event.is_set(): #代表绿灯
            print("[%s] running..."% name )
            time.sleep(1)
        else:
            print("[%s] sees red light , waiting...." %name)
            event.wait()
            print("\033[34;1m[%s] green light is on, start going...\033[0m" %name)


light = threading.Thread(target=lighter,)
light.start()

car1 = threading.Thread(target=car,args=("现代",))
car1.start()
```
#### queue队列：

class queue.Queue(maxsize=0) #先入先出      
class queue.LifoQueue(maxsize=0) #last in fisrt out     
class queue.PriorityQueue(maxsize=0) #存储数据时可设置优先级的队列       
`队列其实就可以比作一个容器`但是列表也是容器,这两个有什么区别吗？     
列表拿出来一个数据做操作其实就是复制了一个数据，原数据还在列表中，但是队列拿出来一个数据就少一个   

queue的主要使用是什么？    
解耦：使程序直接实现松耦合；      
提高处理效率；     
队列是实现解耦的工具，解耦实现的模型就是生产者和消费者模型


- class queue.Queue
```ruby
>>> import queue
>>> q = queue.Queue()  # 先进先出原则
>>> q.put("d1")
>>> q.put("d2")
>>> q.put("d3")
>>> q.get()
'd1'
>>> q.get()
'd2'
>>> q.get()
'd3'
>>> q.get()  # 再取就卡主了


```
为了在获取数据时不让卡主的解决办法：
```ruby
方式1：
# 在最后的一次get中使用get_nowait()

>>> q.get_nowait()  # 抓住下面的错误就行了
Traceback (most recent call last):
  File "<input>", line 1, in <module>
  File "D:\python34\lib\queue.py", line 195, in get_nowait
    return self.get(block=False)
  File "D:\python34\lib\queue.py", line 164, in get
    raise Empty
queue.Empty
```
```ruby
方式2：
# 判断qsize如果等于0了，就不要再get
>>> q.qsize()
0
```
```ruby
方式3：
# 使用get(block=False)判断
>>> q.get(block=False)
Traceback (most recent call last):
  File "<input>", line 1, in <module>
  File "D:\python34\lib\queue.py", line 164, in get
    raise Empty
queue.Empty
```
```ruby
方式4：
# get(timeout=1)如果有卡主，也就是卡主1秒
>>> q.get(timeout=1)
Traceback (most recent call last):
  File "<input>", line 1, in <module>
  File "D:\python34\lib\queue.py", line 175, in get
    raise Empty
queue.Empty
```
也可以设置maxsize大小：
```ruby
>>> import queue
>>> q = queue.Queue(maxsize=3)
>>> q.put(1)
>>> q.put(2)
>>> q.put(2)
>>> q.put(4)  # 卡住了


```
- class queue.LifoQueue：
```ruby
>>> import queue
>>> q = queue.LifoQueue()
>>> q.put(1)
>>> q.put(2)
>>> q.put(3)
>>> print(q.get())
3
>>> print(q.get())
2
>>> print(q.get())
1
```
- class queue.PriorityQueue:
```ruby
import queue

q = queue.PriorityQueue()

q.put((1,"shen"))
q.put((10,"wangzy"))
q.put((5,"baby"))
q.put((-1,"wangzongyu"))

print(q.get())
print(q.get())
print(q.get())
print(q.get())
```
`运行结果：`
```ruby
(-1, 'wangzongyu')
(1, 'shen')
(5, 'baby')
(10, 'wangzy')
```
#### 生产者消费者模型：
```ruby
import threading,time
import queue

q = queue.Queue(maxsize=10)

def Producer(name):
    count = 1
    while True:
        q.put("馒头%s" % count)
        print("生产了馒头",count)
        count +=1
        time.sleep(0.2)

def  Consumer(name):
    while True:
        print("[%s] 取到[%s] 并且吃了它..." %(name, q.get()))
        time.sleep(1)

p = threading.Thread(target=Producer,args=("wangzy",))
c1 = threading.Thread(target=Consumer,args=("baby",))
c2 = threading.Thread(target=Consumer,args=("shen",))

p.start()
c1.start()
c2.start()

```
`运行结果：`
```ruby
生产了馒头 1
[baby] 取到[馒头1] 并且吃了它...
生产了馒头 2
[shen] 取到[馒头2] 并且吃了它...
生产了馒头 3
生产了馒头 4
生产了馒头 5
[baby] 取到[馒头3] 并且吃了它...
生产了馒头 6
[shen] 取到[馒头4] 并且吃了它...
生产了馒头 7
生产了馒头 8
生产了馒头 9
生产了馒头 10
生产了馒头 11
[baby] 取到[馒头5] 并且吃了它...
[shen] 取到[馒头6] 并且吃了它...
生产了馒头 12
生产了馒头 13
......
```

### 多进程
如果是一个8核的CPU，线程的个数可以任意起，但是同时只能运行8个进程(想让进程运行，进程下至少有一个线程)

#### 启动多进程：
```ruby
import multiprocessing
import time

def run(name):
    time.sleep(2)
    print("hello",name)

if __name__ == '__main__':
    for i in range(10):
        p = multiprocessing.Process(target=run, args=('babyshen %s' %i,))
        p.start()
```
- 每个进程中再启动线程：
```ruby
def run(name):
    time.sleep(2)
    print("hello",name)
    t = threading.Thread(target=thread_run,)
    t.start()

def thread_run(): # 定义一个线程
    print(threading.get_ident())

if __name__ == '__main__':
    for i in range(10):
        p = multiprocessing.Process(target=run, args=('babyshen %s' %i,))
        p.start()
```
#### 查看父进程和子进程ID:
```ruby
from multiprocessing import Process
import os

def info(title):
    print(title)
    print('module name:', __name__)
    print('parent process:', os.getppid())  # 获取父进程ID
    print('process id:', os.getpid()) # 输出自己进程的ID
    print("\n\n")

def f(name):
    info('\033[31;1mcalled from child process function f\033[0m')
    print('hello', name)

if __name__ == '__main__':
    info('\033[32;1mmain process line\033[0m')  这个程序本身就是一个进程，执行一个info方法
    p = Process(target=f, args=('babyshen',)) 又起了一个子进程，这个子进程执行f方法，f方法中又调用了info方法
    p.start()
    p.join()
```
`运行结果：`
```ruby
main process line
module name: __main__
parent process: 8036  # 这个是pycharm程序的进程号
process id: 1788

called from child process function f
module name: __mp_main__
parent process: 1788
process id: 8304

hello babyshen
```
#### 进程间通讯：
`Queues和Pipe只是实现进程间数据的传递`
`不同进程间内存是不共享的，要想实现两个进程间的数据交换，可以用以下两种方法`

- Queues(跟线程的Queue有区别，使用方法差不多)
```ruby
线程案例(通过线程对比了解进程间的通讯)
import threading,queue

def f(qq):
    qq.put([100,None,"babyshen"])

if __name__ == '__main__':
    q = queue.Queue()
    p = threading.Thread(target=f,) # 起了一个子线程(父线程是当前程序)
    # p = Process(target=f,args=(q,)) # 换成进程之后，就报错了(q没有定义)，因为父子进程的内存是独立的,就算传个变量q给方法也不行
    p.start()
    print(q.get())    # 父线程get子线程的数据，是可以获取的
    p.join()
#线程的q不能传递给进程的q 


进程(queue要改为进程Queue)
from multiprocessing import Process, Queue

def f(qq):
    qq.put([100, None, 'babyshen'])

if __name__ == '__main__':
    q = Queue()
    p = Process(target=f, args=(q,))
    p.start()
    print(q.get())
    p.join()
#父进程将q传给子进程的q，实则是两个q，两个进程内存里面的数据是无法传递的(进程独立内存)，必须通过pickle序列化一下传递，然后在反序列化接收，并不是修改的同一份数据，而是实现的数据传递
```
- Pipes(管道)
```ruby

from multiprocessing import Process, Pipe

def f(conn):
    conn.send([100, None, 'hello from child1']) # 可以实现多发送
    conn.send([100, None, 'hello from child2'])
    print("from parent:",conn.recv()) # 子接收
    conn.close()

if __name__ == '__main__':
    parent_conn, child_conn = Pipe()  # 管道的两头，一头是父，一头是子
    p = Process(target=f, args=(child_conn,))
    p.start()
    #parent_conn.recv()
    print(parent_conn.recv()) # 可以实现多接收
    print(parent_conn.recv())
    parent_conn.send("hello child") # 父发送
    p.join()
```
`运行结果：`
```ruby
[100, None, 'hello from child1']
[100, None, 'hello from child2']
from parent: hello child

```

#### Managers：
`Manager实现了进程间数据的共享，即多个进程可以修改同一份数据`
```ruby
from multiprocessing import Process, Manager
import os

def f(d, l):
    d[os.getpid()] =os.getpid()  #d[key] = value,key值是进程的pid，value也是进程的pid
    l.append(os.getpid())  # 列表中放入进程的pid

    print("进程内容>>>",l)  # 输出结果说明进程2修改了进程1的内容

if __name__ == '__main__':
    with Manager() as manager:
        d = manager.dict() #{} #生成一个字典，可在多个进程间共享和传递
        l = manager.list(range(5))#生成一个列表，可在多个进程间共享和传递

        p_list = [] # 类似于启动多线程一样，为了使用join，定义一个空列表
        for i in range(10):
            p = Process(target=f, args=(d, l))
            p.start()
            p_list.append(p)

        for res in p_list: #等待结果
            res.join()

        print("最终字典中的内容：",d)
        print("最终列表中的内容",l)

```
`运行结果：`
```ruby
进程内容>>> [0, 1, 2, 3, 4, 9072]
进程内容>>> [0, 1, 2, 3, 4, 9072, 9988]
进程内容>>> [0, 1, 2, 3, 4, 9072, 9988, 8280]
进程内容>>> [0, 1, 2, 3, 4, 9072, 9988, 8280, 8304]
进程内容>>> [0, 1, 2, 3, 4, 9072, 9988, 8280, 8304, 7368]
进程内容>>> [0, 1, 2, 3, 4, 9072, 9988, 8280, 8304, 7368, 7992]
进程内容>>> [0, 1, 2, 3, 4, 9072, 9988, 8280, 8304, 7368, 7992, 8460]
进程内容>>> [0, 1, 2, 3, 4, 9072, 9988, 8280, 8304, 7368, 7992, 8460, 7936]
进程内容>>> [0, 1, 2, 3, 4, 9072, 9988, 8280, 8304, 7368, 7992, 8460, 7936, 6512]
进程内容>>> [0, 1, 2, 3, 4, 9072, 9988, 8280, 8304, 7368, 7992, 8460, 7936, 6512, 7416]
最终字典中的内容： {9072: 9072, 7368: 7368, 9988: 9988, 7416: 7416, 8280: 8280, 8304: 8304, 8460: 8460, 6512: 6512, 7936: 7936, 7992: 7992}
最终列表中的内容 [0, 1, 2, 3, 4, 9072, 9988, 8280, 8304, 7368, 7992, 8460, 7936, 6512, 7416]

# Managers自己内部其实也是copy好几份数据出去，所以不需要加锁
```
- 进程加锁：
`进程锁的目的：为了每个进程打印的时候格式不会错乱，抢占打印输出`
```ruby
from multiprocessing import Process, Lock

def f(l, i):
    l.acquire()
    print('hello world', i)
    l.release()

if __name__ == '__main__':
    lock = Lock()

    for num in range(10):
        Process(target=f, args=(lock, num)).start()
```
#### 进程池
`同一时间，有多少个进程再CPU上运行`
进程池内部维护一个进程序列，当使用时，则去进程池中获取一个进程，如果进程池序列中没有可供使用的进进程，那么程序就会等待，直到进程池中有可用进程为止    
进程池中有两个方法：
- apply :串行
- apply_async：多进程并行

```ruby
from  multiprocessing import Process,Pool,freeze_support
# freeze_support只有在windows上执行才需要导入，因为windows和linux的进程不一样
import time
import os


def Foo(i):
    time.sleep(1)
    print("in process",os.getpid())
    return i+100

def Bar(arg):
    print('-->exec done:',arg,os.getpid())

if __name__ == "__main__":  # windows上必须加入这句
    pool = Pool(5) # 5 = processes=5，允许进程池同时放入5个进程
    # 如果再有很多的进程，其他的进程是启动了，但是还没有交给CPU运行，只有到进程池的CPU才运行
    print("主进程的PID>>>",os.getpid())

    for i in range(10):
        pool.apply_async(func=Foo, args=(i,),callback=Bar) # 并行 5个5个的执行
        # callback是回调，执行完func=Foo，再回调执行callback=Bar的方法
        #pool.apply(func=Foo, args=(i,))

    print('end')
    pool.close() # 一定要先close(),然后再join()
    pool.join() #进程池中进程执行完毕后再关闭，如果注释，那么程序直接关闭。
```

`运行结果：`
```ruby
主进程的PID>>> 10944
end
in process 9420
-->exec done: 100 10944  # 执行回调函数，通过进程号可以看出是主进程执行的回调函数
in process 9432
-->exec done: 101 10944
in process 4568
-->exec done: 102 10944
in process 9372
-->exec done: 103 10944
in process 11228
-->exec done: 104 10944
in process 9420
-->exec done: 105 10944
in process 9432
-->exec done: 106 10944
in process 4568
-->exec done: 107 10944
in process 9372
-->exec done: 108 10944
in process 11228
-->exec done: 109 10944

# 如果是子进程调用回调函数，那么回调函数就没有存在的必要了，功能都直接写在子进程就可以了，所以是主进程调用回调函数
```

`if __name__ == '__main__'`这句话在代码中的作用：
```ruby
main_name.py文件内容：

import multiprocessing
import time

def run(name):
    time.sleep(2)
    print("hello",name)

print(__name__)
if __name__ == '__main__':
    for i in range(10):
        p = multiprocessing.Process(target=run, args=('babyshen %s' %i,))
        p.start()
    print("name_between_main")

print("end")
```
`运行结果：`
```
__main__
__mp_main__
end
__mp_main__
end
......
end
hello babyshen 0
hello babyshen 1
......
```
```ruby
xx.py内容：
import main_name
```

`运行结果：`
```ruby
main_name  # 这就输出了import的模块名
end
```
` 手动执行的时候，__name__ == '__main__'下的代码都会执行，但是假如通过import导入的时候，__name__ == '__main__'下的代码不会被执行，前面以及后面的代码会被执行`



### 协成
作用：`遇到IO操作就切换`
需要安装gevent模块
```ruby
D:\python34>pip install gevent-1.2.2-cp34-cp34m-win32.whl
Processing d:\python34\gevent-1.2.2-cp34-cp34m-win32.whl
Collecting greenlet>=0.4.10 (from gevent==1.2.2)
  Downloading greenlet-0.4.12-cp34-cp34m-win32.whl
Installing collected packages: greenlet, gevent
Successfully installed gevent-1.2.2 greenlet-0.4.12

# python2.7就选择cp27,intel平台的就选择win32,而win-amd64不是对应的win64
```
`gevent是对greenlet的再封装，greenlat是手动切换，gevent是自动切换`

- greenlet

```ruby

from greenlet import greenlet

def test1():
    print(12)    #第2步
    gr2.switch() #第3步，切换到test2方法
    print(34)     #第6步
    gr2.switch()  #第7步

def test2():
    print(56)   #第4步
    gr1.switch() #第5步，切换回到第3步之下
    print(78)    #第8步

gr1 = greenlet(test1)
gr2 = greenlet(test2)
gr1.switch()   # 第1步，切换到gr1，也就是test1方法
```
- gevent
```ruby
import gevent
import time

def foo():
    print('Begin gevent in the foo')
    global start_time
    start_time = time.time()
    gevent.sleep(2)
    print('End gevent in the foo')
    print("total end run bar-time",time.time() - start_time)
def bar():
    print('begin gevent in the bar')
    gevent.sleep(1)
    print("total begin run bar-time",time.time() - start_time)
    print('end gevent in the bar')
    print("total end run bar-time",time.time() - start_time)


def func3():
    print("Begin gevent in the func3 ")
    gevent.sleep(0)
    print("End gevent in the func3 ")
    print("total end run func3-time",time.time() - start_time)


gevent.joinall([
    gevent.spawn(foo), 
    gevent.spawn(bar),
    gevent.spawn(func3),
])
```
`运行结果：`
```ruby
Begin gevent in the foo
begin gevent in the bar
Begin gevent in the func3 
End gevent in the func3 
total end run func3-time 0.0  # 上面这几行瞬间输出，输出下一行会隔1秒
total begin run bar-time 1.0000569820404053
end gevent in the bar
total end run bar-time 1.0000569820404053  # 从这到下面这行会卡顿1秒
End gevent in the foo
total end run bar-time 2.0001139640808105
```
- 使用协程练习网络爬虫案例一：
```ruby
from urllib import request
import gevent,time
from gevent import monkey  # 打个补丁，否则下面的异步也是串行的
monkey.patch_all() #把当前程序的所有的io操作给我,单独的做上标记

print("-----------串行爬虫一下计算时间--------------------")
def f(url):
    print('GET: %s' % url)
    resp = request.urlopen(url)
    data = resp.read()
    print('%d bytes received from %s.' % (len(data), url))

urls = ['https://www.python.org/',
        'https://www.yahoo.com/',
        'https://github.com/' ]
time_start = time.time()
for url in urls:
    f(url)
print("同步cost",time.time() - time_start)
print("---------------------串行end------------------------")

print("-------------并行爬虫一下计算时间--------------------")
async_time_start = time.time()
gevent.joinall([
    gevent.spawn(f, 'https://www.python.org/'),
    gevent.spawn(f, 'https://www.yahoo.com/'),
    gevent.spawn(f, 'https://github.com/'),
])
print("异步cost",time.time() - async_time_start)
print("-------------------并行 end--------------------------")
```
`运行结果：`
```ruby
-----------串行爬虫一下计算时间--------------------
GET: https://www.python.org/
48956 bytes received from https://www.python.org/.
GET: https://www.yahoo.com/
508909 bytes received from https://www.yahoo.com/.
GET: https://github.com/
51489 bytes received from https://github.com/.
同步cost 5.233299016952515
---------------------串行end------------------------
-------------并行爬虫一下计算时间--------------------
GET: https://www.python.org/
GET: https://www.yahoo.com/
GET: https://github.com/
48956 bytes received from https://www.python.org/.
508844 bytes received from https://www.yahoo.com/.
51489 bytes received from https://github.com/.
异步cost 2.865164041519165
-------------------并行 end--------------------------
```
- 一个Socket+协程实现多并发：
```ruby

```
### 总结：
- 多进程能够利用多核优势，但是进程间通信比较麻烦，另外，进程数目的增加会使性能下降，进程切换的成本较高。程序流程复杂度相对I/O多路复用要低。
- I/O多路复用是在一个进程内部处理多个逻辑流程，不用进行进程切换，性能较高，另外流程间共享信息简单。但是无法利用多核优势，另外，程序流程被事件处理切割成一个个小块，程序比较复杂，难于理解。
- 线程运行在一个进程内部，由操作系统调度，切换成本较低，另外，他们共享进程的虚拟地址空间，线程间共享信息简单。但是线程安全问题导致线程学习曲线陡峭，而且易出错。
- 协程有编程语言提供，由程序员控制进行切换，所以没有线程安全问题，可以用来处理状态机，并发请求等。但是无法利用多核优势。
上面的四种方案可以配合使用，我比较看好的是进程+协程的模式。

- greenlet协程的运行，其本质是串行的，所以它不是真正意义上的并发，因此也无法发挥CPU多核的优势，不过，这个可以通过协程+进程组合的方式来解决